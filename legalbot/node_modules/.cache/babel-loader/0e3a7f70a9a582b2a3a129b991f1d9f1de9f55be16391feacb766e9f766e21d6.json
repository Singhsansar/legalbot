{"ast":null,"code":"import { AIMessage, AIMessageChunk, ChatMessage, isBaseMessage } from \"@langchain/core/messages\";\nimport { ChatGenerationChunk } from \"@langchain/core/outputs\";\nexport function getMessageAuthor(message) {\n  const type = message._getType();\n  if (ChatMessage.isInstance(message)) {\n    return message.role;\n  }\n  return message.name ?? type;\n}\n/**\n * Maps a message type to a Google Generative AI chat author.\n * @param message The message to map.\n * @param model The model to use for mapping.\n * @returns The message type mapped to a Google Generative AI chat author.\n */\nexport function convertAuthorToRole(author) {\n  switch (author) {\n    /**\n     *  Note: Gemini currently is not supporting system messages\n     *  we will convert them to human messages and merge with following\n     * */\n    case \"ai\":\n    case \"model\":\n      // getMessageAuthor returns message.name. code ex.: return message.name ?? type;\n      return \"model\";\n    case \"system\":\n    case \"human\":\n      return \"user\";\n    default:\n      throw new Error(`Unknown / unsupported author: ${author}`);\n  }\n}\nexport function convertMessageContentToParts(content, isMultimodalModel) {\n  if (typeof content === \"string\") {\n    return [{\n      text: content\n    }];\n  }\n  return content.map(c => {\n    if (c.type === \"text\") {\n      return {\n        text: c.text\n      };\n    }\n    if (c.type === \"image_url\") {\n      if (!isMultimodalModel) {\n        throw new Error(`This model does not support images`);\n      }\n      if (typeof c.image_url !== \"string\") {\n        throw new Error(\"Please provide image as base64 encoded data URL\");\n      }\n      const [dm, data] = c.image_url.split(\",\");\n      if (!dm.startsWith(\"data:\")) {\n        throw new Error(\"Please provide image as base64 encoded data URL\");\n      }\n      const [mimeType, encoding] = dm.replace(/^data:/, \"\").split(\";\");\n      if (encoding !== \"base64\") {\n        throw new Error(\"Please provide image as base64 encoded data URL\");\n      }\n      return {\n        inlineData: {\n          data,\n          mimeType\n        }\n      };\n    }\n    throw new Error(`Unknown content type ${c.type}`);\n  });\n}\nexport function convertBaseMessagesToContent(messages, isMultimodalModel) {\n  return messages.reduce((acc, message, index) => {\n    if (!isBaseMessage(message)) {\n      throw new Error(\"Unsupported message input\");\n    }\n    const author = getMessageAuthor(message);\n    if (author === \"system\" && index !== 0) {\n      throw new Error(\"System message should be the first one\");\n    }\n    const role = convertAuthorToRole(author);\n    const prevContent = acc.content[acc.content.length];\n    if (!acc.mergeWithPreviousContent && prevContent && prevContent.role === role) {\n      throw new Error(\"Google Generative AI requires alternate messages between authors\");\n    }\n    const parts = convertMessageContentToParts(message.content, isMultimodalModel);\n    if (acc.mergeWithPreviousContent) {\n      const prevContent = acc.content[acc.content.length - 1];\n      if (!prevContent) {\n        throw new Error(\"There was a problem parsing your system message. Please try a prompt without one.\");\n      }\n      prevContent.parts.push(...parts);\n      return {\n        mergeWithPreviousContent: false,\n        content: acc.content\n      };\n    }\n    const content = {\n      role,\n      parts\n    };\n    return {\n      mergeWithPreviousContent: author === \"system\",\n      content: [...acc.content, content]\n    };\n  }, {\n    content: [],\n    mergeWithPreviousContent: false\n  }).content;\n}\nexport function mapGenerateContentResultToChatResult(response) {\n  // if rejected or error, return empty generations with reason in filters\n  if (!response.candidates || response.candidates.length === 0 || !response.candidates[0]) {\n    return {\n      generations: [],\n      llmOutput: {\n        filters: response.promptFeedback\n      }\n    };\n  }\n  const [candidate] = response.candidates;\n  const {\n    content,\n    ...generationInfo\n  } = candidate;\n  const text = content?.parts[0]?.text ?? \"\";\n  const generation = {\n    text,\n    message: new AIMessage({\n      content: text,\n      name: !content ? undefined : content.role,\n      additional_kwargs: generationInfo\n    }),\n    generationInfo\n  };\n  return {\n    generations: [generation]\n  };\n}\nexport function convertResponseContentToChatGenerationChunk(response) {\n  if (!response.candidates || response.candidates.length === 0) {\n    return null;\n  }\n  const [candidate] = response.candidates;\n  const {\n    content,\n    ...generationInfo\n  } = candidate;\n  const text = content?.parts[0]?.text ?? \"\";\n  return new ChatGenerationChunk({\n    text,\n    message: new AIMessageChunk({\n      content: text,\n      name: !content ? undefined : content.role,\n      // Each chunk can have unique \"generationInfo\", and merging strategy is unclear,\n      // so leave blank for now.\n      additional_kwargs: {}\n    }),\n    generationInfo\n  });\n}","map":{"version":3,"names":["AIMessage","AIMessageChunk","ChatMessage","isBaseMessage","ChatGenerationChunk","getMessageAuthor","message","type","_getType","isInstance","role","name","convertAuthorToRole","author","Error","convertMessageContentToParts","content","isMultimodalModel","text","map","c","image_url","dm","data","split","startsWith","mimeType","encoding","replace","inlineData","convertBaseMessagesToContent","messages","reduce","acc","index","prevContent","length","mergeWithPreviousContent","parts","push","mapGenerateContentResultToChatResult","response","candidates","generations","llmOutput","filters","promptFeedback","candidate","generationInfo","generation","undefined","additional_kwargs","convertResponseContentToChatGenerationChunk"],"sources":["/home/nikhil/legalbot/legalbot/node_modules/@langchain/google-genai/dist/utils.js"],"sourcesContent":["import { AIMessage, AIMessageChunk, ChatMessage, isBaseMessage, } from \"@langchain/core/messages\";\nimport { ChatGenerationChunk, } from \"@langchain/core/outputs\";\nexport function getMessageAuthor(message) {\n    const type = message._getType();\n    if (ChatMessage.isInstance(message)) {\n        return message.role;\n    }\n    return message.name ?? type;\n}\n/**\n * Maps a message type to a Google Generative AI chat author.\n * @param message The message to map.\n * @param model The model to use for mapping.\n * @returns The message type mapped to a Google Generative AI chat author.\n */\nexport function convertAuthorToRole(author) {\n    switch (author) {\n        /**\n         *  Note: Gemini currently is not supporting system messages\n         *  we will convert them to human messages and merge with following\n         * */\n        case \"ai\":\n        case \"model\": // getMessageAuthor returns message.name. code ex.: return message.name ?? type;\n            return \"model\";\n        case \"system\":\n        case \"human\":\n            return \"user\";\n        default:\n            throw new Error(`Unknown / unsupported author: ${author}`);\n    }\n}\nexport function convertMessageContentToParts(content, isMultimodalModel) {\n    if (typeof content === \"string\") {\n        return [{ text: content }];\n    }\n    return content.map((c) => {\n        if (c.type === \"text\") {\n            return {\n                text: c.text,\n            };\n        }\n        if (c.type === \"image_url\") {\n            if (!isMultimodalModel) {\n                throw new Error(`This model does not support images`);\n            }\n            if (typeof c.image_url !== \"string\") {\n                throw new Error(\"Please provide image as base64 encoded data URL\");\n            }\n            const [dm, data] = c.image_url.split(\",\");\n            if (!dm.startsWith(\"data:\")) {\n                throw new Error(\"Please provide image as base64 encoded data URL\");\n            }\n            const [mimeType, encoding] = dm.replace(/^data:/, \"\").split(\";\");\n            if (encoding !== \"base64\") {\n                throw new Error(\"Please provide image as base64 encoded data URL\");\n            }\n            return {\n                inlineData: {\n                    data,\n                    mimeType,\n                },\n            };\n        }\n        throw new Error(`Unknown content type ${c.type}`);\n    });\n}\nexport function convertBaseMessagesToContent(messages, isMultimodalModel) {\n    return messages.reduce((acc, message, index) => {\n        if (!isBaseMessage(message)) {\n            throw new Error(\"Unsupported message input\");\n        }\n        const author = getMessageAuthor(message);\n        if (author === \"system\" && index !== 0) {\n            throw new Error(\"System message should be the first one\");\n        }\n        const role = convertAuthorToRole(author);\n        const prevContent = acc.content[acc.content.length];\n        if (!acc.mergeWithPreviousContent &&\n            prevContent &&\n            prevContent.role === role) {\n            throw new Error(\"Google Generative AI requires alternate messages between authors\");\n        }\n        const parts = convertMessageContentToParts(message.content, isMultimodalModel);\n        if (acc.mergeWithPreviousContent) {\n            const prevContent = acc.content[acc.content.length - 1];\n            if (!prevContent) {\n                throw new Error(\"There was a problem parsing your system message. Please try a prompt without one.\");\n            }\n            prevContent.parts.push(...parts);\n            return {\n                mergeWithPreviousContent: false,\n                content: acc.content,\n            };\n        }\n        const content = {\n            role,\n            parts,\n        };\n        return {\n            mergeWithPreviousContent: author === \"system\",\n            content: [...acc.content, content],\n        };\n    }, { content: [], mergeWithPreviousContent: false }).content;\n}\nexport function mapGenerateContentResultToChatResult(response) {\n    // if rejected or error, return empty generations with reason in filters\n    if (!response.candidates ||\n        response.candidates.length === 0 ||\n        !response.candidates[0]) {\n        return {\n            generations: [],\n            llmOutput: {\n                filters: response.promptFeedback,\n            },\n        };\n    }\n    const [candidate] = response.candidates;\n    const { content, ...generationInfo } = candidate;\n    const text = content?.parts[0]?.text ?? \"\";\n    const generation = {\n        text,\n        message: new AIMessage({\n            content: text,\n            name: !content ? undefined : content.role,\n            additional_kwargs: generationInfo,\n        }),\n        generationInfo,\n    };\n    return {\n        generations: [generation],\n    };\n}\nexport function convertResponseContentToChatGenerationChunk(response) {\n    if (!response.candidates || response.candidates.length === 0) {\n        return null;\n    }\n    const [candidate] = response.candidates;\n    const { content, ...generationInfo } = candidate;\n    const text = content?.parts[0]?.text ?? \"\";\n    return new ChatGenerationChunk({\n        text,\n        message: new AIMessageChunk({\n            content: text,\n            name: !content ? undefined : content.role,\n            // Each chunk can have unique \"generationInfo\", and merging strategy is unclear,\n            // so leave blank for now.\n            additional_kwargs: {},\n        }),\n        generationInfo,\n    });\n}\n"],"mappings":"AAAA,SAASA,SAAS,EAAEC,cAAc,EAAEC,WAAW,EAAEC,aAAa,QAAS,0BAA0B;AACjG,SAASC,mBAAmB,QAAS,yBAAyB;AAC9D,OAAO,SAASC,gBAAgBA,CAACC,OAAO,EAAE;EACtC,MAAMC,IAAI,GAAGD,OAAO,CAACE,QAAQ,CAAC,CAAC;EAC/B,IAAIN,WAAW,CAACO,UAAU,CAACH,OAAO,CAAC,EAAE;IACjC,OAAOA,OAAO,CAACI,IAAI;EACvB;EACA,OAAOJ,OAAO,CAACK,IAAI,IAAIJ,IAAI;AAC/B;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,SAASK,mBAAmBA,CAACC,MAAM,EAAE;EACxC,QAAQA,MAAM;IACV;AACR;AACA;AACA;IACQ,KAAK,IAAI;IACT,KAAK,OAAO;MAAE;MACV,OAAO,OAAO;IAClB,KAAK,QAAQ;IACb,KAAK,OAAO;MACR,OAAO,MAAM;IACjB;MACI,MAAM,IAAIC,KAAK,CAAE,iCAAgCD,MAAO,EAAC,CAAC;EAClE;AACJ;AACA,OAAO,SAASE,4BAA4BA,CAACC,OAAO,EAAEC,iBAAiB,EAAE;EACrE,IAAI,OAAOD,OAAO,KAAK,QAAQ,EAAE;IAC7B,OAAO,CAAC;MAAEE,IAAI,EAAEF;IAAQ,CAAC,CAAC;EAC9B;EACA,OAAOA,OAAO,CAACG,GAAG,CAAEC,CAAC,IAAK;IACtB,IAAIA,CAAC,CAACb,IAAI,KAAK,MAAM,EAAE;MACnB,OAAO;QACHW,IAAI,EAAEE,CAAC,CAACF;MACZ,CAAC;IACL;IACA,IAAIE,CAAC,CAACb,IAAI,KAAK,WAAW,EAAE;MACxB,IAAI,CAACU,iBAAiB,EAAE;QACpB,MAAM,IAAIH,KAAK,CAAE,oCAAmC,CAAC;MACzD;MACA,IAAI,OAAOM,CAAC,CAACC,SAAS,KAAK,QAAQ,EAAE;QACjC,MAAM,IAAIP,KAAK,CAAC,iDAAiD,CAAC;MACtE;MACA,MAAM,CAACQ,EAAE,EAAEC,IAAI,CAAC,GAAGH,CAAC,CAACC,SAAS,CAACG,KAAK,CAAC,GAAG,CAAC;MACzC,IAAI,CAACF,EAAE,CAACG,UAAU,CAAC,OAAO,CAAC,EAAE;QACzB,MAAM,IAAIX,KAAK,CAAC,iDAAiD,CAAC;MACtE;MACA,MAAM,CAACY,QAAQ,EAAEC,QAAQ,CAAC,GAAGL,EAAE,CAACM,OAAO,CAAC,QAAQ,EAAE,EAAE,CAAC,CAACJ,KAAK,CAAC,GAAG,CAAC;MAChE,IAAIG,QAAQ,KAAK,QAAQ,EAAE;QACvB,MAAM,IAAIb,KAAK,CAAC,iDAAiD,CAAC;MACtE;MACA,OAAO;QACHe,UAAU,EAAE;UACRN,IAAI;UACJG;QACJ;MACJ,CAAC;IACL;IACA,MAAM,IAAIZ,KAAK,CAAE,wBAAuBM,CAAC,CAACb,IAAK,EAAC,CAAC;EACrD,CAAC,CAAC;AACN;AACA,OAAO,SAASuB,4BAA4BA,CAACC,QAAQ,EAAEd,iBAAiB,EAAE;EACtE,OAAOc,QAAQ,CAACC,MAAM,CAAC,CAACC,GAAG,EAAE3B,OAAO,EAAE4B,KAAK,KAAK;IAC5C,IAAI,CAAC/B,aAAa,CAACG,OAAO,CAAC,EAAE;MACzB,MAAM,IAAIQ,KAAK,CAAC,2BAA2B,CAAC;IAChD;IACA,MAAMD,MAAM,GAAGR,gBAAgB,CAACC,OAAO,CAAC;IACxC,IAAIO,MAAM,KAAK,QAAQ,IAAIqB,KAAK,KAAK,CAAC,EAAE;MACpC,MAAM,IAAIpB,KAAK,CAAC,wCAAwC,CAAC;IAC7D;IACA,MAAMJ,IAAI,GAAGE,mBAAmB,CAACC,MAAM,CAAC;IACxC,MAAMsB,WAAW,GAAGF,GAAG,CAACjB,OAAO,CAACiB,GAAG,CAACjB,OAAO,CAACoB,MAAM,CAAC;IACnD,IAAI,CAACH,GAAG,CAACI,wBAAwB,IAC7BF,WAAW,IACXA,WAAW,CAACzB,IAAI,KAAKA,IAAI,EAAE;MAC3B,MAAM,IAAII,KAAK,CAAC,kEAAkE,CAAC;IACvF;IACA,MAAMwB,KAAK,GAAGvB,4BAA4B,CAACT,OAAO,CAACU,OAAO,EAAEC,iBAAiB,CAAC;IAC9E,IAAIgB,GAAG,CAACI,wBAAwB,EAAE;MAC9B,MAAMF,WAAW,GAAGF,GAAG,CAACjB,OAAO,CAACiB,GAAG,CAACjB,OAAO,CAACoB,MAAM,GAAG,CAAC,CAAC;MACvD,IAAI,CAACD,WAAW,EAAE;QACd,MAAM,IAAIrB,KAAK,CAAC,mFAAmF,CAAC;MACxG;MACAqB,WAAW,CAACG,KAAK,CAACC,IAAI,CAAC,GAAGD,KAAK,CAAC;MAChC,OAAO;QACHD,wBAAwB,EAAE,KAAK;QAC/BrB,OAAO,EAAEiB,GAAG,CAACjB;MACjB,CAAC;IACL;IACA,MAAMA,OAAO,GAAG;MACZN,IAAI;MACJ4B;IACJ,CAAC;IACD,OAAO;MACHD,wBAAwB,EAAExB,MAAM,KAAK,QAAQ;MAC7CG,OAAO,EAAE,CAAC,GAAGiB,GAAG,CAACjB,OAAO,EAAEA,OAAO;IACrC,CAAC;EACL,CAAC,EAAE;IAAEA,OAAO,EAAE,EAAE;IAAEqB,wBAAwB,EAAE;EAAM,CAAC,CAAC,CAACrB,OAAO;AAChE;AACA,OAAO,SAASwB,oCAAoCA,CAACC,QAAQ,EAAE;EAC3D;EACA,IAAI,CAACA,QAAQ,CAACC,UAAU,IACpBD,QAAQ,CAACC,UAAU,CAACN,MAAM,KAAK,CAAC,IAChC,CAACK,QAAQ,CAACC,UAAU,CAAC,CAAC,CAAC,EAAE;IACzB,OAAO;MACHC,WAAW,EAAE,EAAE;MACfC,SAAS,EAAE;QACPC,OAAO,EAAEJ,QAAQ,CAACK;MACtB;IACJ,CAAC;EACL;EACA,MAAM,CAACC,SAAS,CAAC,GAAGN,QAAQ,CAACC,UAAU;EACvC,MAAM;IAAE1B,OAAO;IAAE,GAAGgC;EAAe,CAAC,GAAGD,SAAS;EAChD,MAAM7B,IAAI,GAAGF,OAAO,EAAEsB,KAAK,CAAC,CAAC,CAAC,EAAEpB,IAAI,IAAI,EAAE;EAC1C,MAAM+B,UAAU,GAAG;IACf/B,IAAI;IACJZ,OAAO,EAAE,IAAIN,SAAS,CAAC;MACnBgB,OAAO,EAAEE,IAAI;MACbP,IAAI,EAAE,CAACK,OAAO,GAAGkC,SAAS,GAAGlC,OAAO,CAACN,IAAI;MACzCyC,iBAAiB,EAAEH;IACvB,CAAC,CAAC;IACFA;EACJ,CAAC;EACD,OAAO;IACHL,WAAW,EAAE,CAACM,UAAU;EAC5B,CAAC;AACL;AACA,OAAO,SAASG,2CAA2CA,CAACX,QAAQ,EAAE;EAClE,IAAI,CAACA,QAAQ,CAACC,UAAU,IAAID,QAAQ,CAACC,UAAU,CAACN,MAAM,KAAK,CAAC,EAAE;IAC1D,OAAO,IAAI;EACf;EACA,MAAM,CAACW,SAAS,CAAC,GAAGN,QAAQ,CAACC,UAAU;EACvC,MAAM;IAAE1B,OAAO;IAAE,GAAGgC;EAAe,CAAC,GAAGD,SAAS;EAChD,MAAM7B,IAAI,GAAGF,OAAO,EAAEsB,KAAK,CAAC,CAAC,CAAC,EAAEpB,IAAI,IAAI,EAAE;EAC1C,OAAO,IAAId,mBAAmB,CAAC;IAC3Bc,IAAI;IACJZ,OAAO,EAAE,IAAIL,cAAc,CAAC;MACxBe,OAAO,EAAEE,IAAI;MACbP,IAAI,EAAE,CAACK,OAAO,GAAGkC,SAAS,GAAGlC,OAAO,CAACN,IAAI;MACzC;MACA;MACAyC,iBAAiB,EAAE,CAAC;IACxB,CAAC,CAAC;IACFH;EACJ,CAAC,CAAC;AACN","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}