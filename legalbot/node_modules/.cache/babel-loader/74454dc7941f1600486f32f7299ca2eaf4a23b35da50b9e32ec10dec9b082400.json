{"ast":null,"code":"import { GoogleGenerativeAI as GenerativeAI } from \"@google/generative-ai\";\nimport { getEnvironmentVariable } from \"@langchain/core/utils/env\";\nimport { BaseChatModel } from \"@langchain/core/language_models/chat_models\";\nimport { convertBaseMessagesToContent, convertResponseContentToChatGenerationChunk, mapGenerateContentResultToChatResult } from \"./utils.js\";\n/**\n * A class that wraps the Google Palm chat model.\n * @example\n * ```typescript\n * const model = new ChatGoogleGenerativeAI({\n *   apiKey: \"<YOUR API KEY>\",\n *   temperature: 0.7,\n *   modelName: \"gemini-pro\",\n *   topK: 40,\n *   topP: 1,\n * });\n * const questions = [\n *   new HumanMessage({\n *     content: [\n *       {\n *         type: \"text\",\n *         text: \"You are a funny assistant that answers in pirate language.\",\n *       },\n *       {\n *         type: \"text\",\n *         text: \"What is your favorite food?\",\n *       },\n *     ]\n *   })\n * ];\n * const res = await model.call(questions);\n * console.log({ res });\n * ```\n */\nexport class ChatGoogleGenerativeAI extends BaseChatModel {\n  static lc_name() {\n    return \"googlegenerativeai\";\n  }\n  get lc_secrets() {\n    return {\n      apiKey: \"GOOGLE_API_KEY\"\n    };\n  }\n  get _isMultimodalModel() {\n    return this.modelName.includes(\"vision\");\n  }\n  constructor(fields) {\n    super(fields ?? {});\n    Object.defineProperty(this, \"lc_serializable\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: true\n    });\n    Object.defineProperty(this, \"modelName\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: \"gemini-pro\"\n    });\n    Object.defineProperty(this, \"temperature\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    }); // default value chosen based on model\n    Object.defineProperty(this, \"maxOutputTokens\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"topP\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    }); // default value chosen based on model\n    Object.defineProperty(this, \"topK\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    }); // default value chosen based on model\n    Object.defineProperty(this, \"stopSequences\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: []\n    });\n    Object.defineProperty(this, \"safetySettings\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"apiKey\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"streaming\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: false\n    });\n    Object.defineProperty(this, \"client\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    this.modelName = fields?.modelName?.replace(/^models\\//, \"\") ?? this.modelName;\n    this.maxOutputTokens = fields?.maxOutputTokens ?? this.maxOutputTokens;\n    if (this.maxOutputTokens && this.maxOutputTokens < 0) {\n      throw new Error(\"`maxOutputTokens` must be a positive integer\");\n    }\n    this.temperature = fields?.temperature ?? this.temperature;\n    if (this.temperature && (this.temperature < 0 || this.temperature > 1)) {\n      throw new Error(\"`temperature` must be in the range of [0.0,1.0]\");\n    }\n    this.topP = fields?.topP ?? this.topP;\n    if (this.topP && this.topP < 0) {\n      throw new Error(\"`topP` must be a positive integer\");\n    }\n    if (this.topP && this.topP > 1) {\n      throw new Error(\"`topP` must be below 1.\");\n    }\n    this.topK = fields?.topK ?? this.topK;\n    if (this.topK && this.topK < 0) {\n      throw new Error(\"`topK` must be a positive integer\");\n    }\n    this.stopSequences = fields?.stopSequences ?? this.stopSequences;\n    this.apiKey = fields?.apiKey ?? getEnvironmentVariable(\"GOOGLE_API_KEY\");\n    if (!this.apiKey) {\n      throw new Error(\"Please set an API key for Google GenerativeAI \" + \"in the environment variable GOOGLE_API_KEY \" + \"or in the `apiKey` field of the \" + \"ChatGoogleGenerativeAI constructor\");\n    }\n    this.safetySettings = fields?.safetySettings ?? this.safetySettings;\n    if (this.safetySettings && this.safetySettings.length > 0) {\n      const safetySettingsSet = new Set(this.safetySettings.map(s => s.category));\n      if (safetySettingsSet.size !== this.safetySettings.length) {\n        throw new Error(\"The categories in `safetySettings` array must be unique\");\n      }\n    }\n    this.streaming = fields?.streaming ?? this.streaming;\n    this.client = new GenerativeAI(this.apiKey).getGenerativeModel({\n      model: this.modelName,\n      safetySettings: this.safetySettings,\n      generationConfig: {\n        candidateCount: 1,\n        stopSequences: this.stopSequences,\n        maxOutputTokens: this.maxOutputTokens,\n        temperature: this.temperature,\n        topP: this.topP,\n        topK: this.topK\n      }\n    });\n  }\n  _combineLLMOutput() {\n    return [];\n  }\n  _llmType() {\n    return \"googlegenerativeai\";\n  }\n  async _generate(messages, options, runManager) {\n    const prompt = convertBaseMessagesToContent(messages, this._isMultimodalModel);\n    // Handle streaming\n    if (this.streaming) {\n      const tokenUsage = {};\n      const stream = this._streamResponseChunks(messages, options, runManager);\n      const finalChunks = {};\n      for await (const chunk of stream) {\n        const index = chunk.generationInfo?.completion ?? 0;\n        if (finalChunks[index] === undefined) {\n          finalChunks[index] = chunk;\n        } else {\n          finalChunks[index] = finalChunks[index].concat(chunk);\n        }\n      }\n      const generations = Object.entries(finalChunks).sort(([aKey], [bKey]) => parseInt(aKey, 10) - parseInt(bKey, 10)).map(([_, value]) => value);\n      return {\n        generations,\n        llmOutput: {\n          estimatedTokenUsage: tokenUsage\n        }\n      };\n    }\n    const res = await this.caller.callWithOptions({\n      signal: options?.signal\n    }, async () => {\n      let output;\n      try {\n        output = await this.client.generateContent({\n          contents: prompt\n        });\n        // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      } catch (e) {\n        // TODO: Improve error handling\n        if (e.message?.includes(\"400 Bad Request\")) {\n          e.status = 400;\n        }\n        throw e;\n      }\n      return output;\n    });\n    const generationResult = mapGenerateContentResultToChatResult(res.response);\n    await runManager?.handleLLMNewToken(generationResult.generations[0].text ?? \"\");\n    return generationResult;\n  }\n  async *_streamResponseChunks(messages, options, runManager) {\n    const prompt = convertBaseMessagesToContent(messages, this._isMultimodalModel);\n    const stream = await this.caller.callWithOptions({\n      signal: options?.signal\n    }, async () => {\n      const {\n        stream\n      } = await this.client.generateContentStream({\n        contents: prompt\n      });\n      return stream;\n    });\n    for await (const response of stream) {\n      const chunk = convertResponseContentToChatGenerationChunk(response);\n      if (!chunk) {\n        continue;\n      }\n      yield chunk;\n      await runManager?.handleLLMNewToken(chunk.text ?? \"\");\n    }\n  }\n}","map":{"version":3,"names":["GoogleGenerativeAI","GenerativeAI","getEnvironmentVariable","BaseChatModel","convertBaseMessagesToContent","convertResponseContentToChatGenerationChunk","mapGenerateContentResultToChatResult","ChatGoogleGenerativeAI","lc_name","lc_secrets","apiKey","_isMultimodalModel","modelName","includes","constructor","fields","Object","defineProperty","enumerable","configurable","writable","value","replace","maxOutputTokens","Error","temperature","topP","topK","stopSequences","safetySettings","length","safetySettingsSet","Set","map","s","category","size","streaming","client","getGenerativeModel","model","generationConfig","candidateCount","_combineLLMOutput","_llmType","_generate","messages","options","runManager","prompt","tokenUsage","stream","_streamResponseChunks","finalChunks","chunk","index","generationInfo","completion","undefined","concat","generations","entries","sort","aKey","bKey","parseInt","_","llmOutput","estimatedTokenUsage","res","caller","callWithOptions","signal","output","generateContent","contents","e","message","status","generationResult","response","handleLLMNewToken","text","generateContentStream"],"sources":["/home/nikhil/legalbot/legalbot/node_modules/@langchain/google-genai/dist/chat_models.js"],"sourcesContent":["import { GoogleGenerativeAI as GenerativeAI, } from \"@google/generative-ai\";\nimport { getEnvironmentVariable } from \"@langchain/core/utils/env\";\nimport { BaseChatModel, } from \"@langchain/core/language_models/chat_models\";\nimport { convertBaseMessagesToContent, convertResponseContentToChatGenerationChunk, mapGenerateContentResultToChatResult, } from \"./utils.js\";\n/**\n * A class that wraps the Google Palm chat model.\n * @example\n * ```typescript\n * const model = new ChatGoogleGenerativeAI({\n *   apiKey: \"<YOUR API KEY>\",\n *   temperature: 0.7,\n *   modelName: \"gemini-pro\",\n *   topK: 40,\n *   topP: 1,\n * });\n * const questions = [\n *   new HumanMessage({\n *     content: [\n *       {\n *         type: \"text\",\n *         text: \"You are a funny assistant that answers in pirate language.\",\n *       },\n *       {\n *         type: \"text\",\n *         text: \"What is your favorite food?\",\n *       },\n *     ]\n *   })\n * ];\n * const res = await model.call(questions);\n * console.log({ res });\n * ```\n */\nexport class ChatGoogleGenerativeAI extends BaseChatModel {\n    static lc_name() {\n        return \"googlegenerativeai\";\n    }\n    get lc_secrets() {\n        return {\n            apiKey: \"GOOGLE_API_KEY\",\n        };\n    }\n    get _isMultimodalModel() {\n        return this.modelName.includes(\"vision\");\n    }\n    constructor(fields) {\n        super(fields ?? {});\n        Object.defineProperty(this, \"lc_serializable\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: true\n        });\n        Object.defineProperty(this, \"modelName\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: \"gemini-pro\"\n        });\n        Object.defineProperty(this, \"temperature\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        }); // default value chosen based on model\n        Object.defineProperty(this, \"maxOutputTokens\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"topP\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        }); // default value chosen based on model\n        Object.defineProperty(this, \"topK\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        }); // default value chosen based on model\n        Object.defineProperty(this, \"stopSequences\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: []\n        });\n        Object.defineProperty(this, \"safetySettings\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"apiKey\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"streaming\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: false\n        });\n        Object.defineProperty(this, \"client\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        this.modelName =\n            fields?.modelName?.replace(/^models\\//, \"\") ?? this.modelName;\n        this.maxOutputTokens = fields?.maxOutputTokens ?? this.maxOutputTokens;\n        if (this.maxOutputTokens && this.maxOutputTokens < 0) {\n            throw new Error(\"`maxOutputTokens` must be a positive integer\");\n        }\n        this.temperature = fields?.temperature ?? this.temperature;\n        if (this.temperature && (this.temperature < 0 || this.temperature > 1)) {\n            throw new Error(\"`temperature` must be in the range of [0.0,1.0]\");\n        }\n        this.topP = fields?.topP ?? this.topP;\n        if (this.topP && this.topP < 0) {\n            throw new Error(\"`topP` must be a positive integer\");\n        }\n        if (this.topP && this.topP > 1) {\n            throw new Error(\"`topP` must be below 1.\");\n        }\n        this.topK = fields?.topK ?? this.topK;\n        if (this.topK && this.topK < 0) {\n            throw new Error(\"`topK` must be a positive integer\");\n        }\n        this.stopSequences = fields?.stopSequences ?? this.stopSequences;\n        this.apiKey = fields?.apiKey ?? getEnvironmentVariable(\"GOOGLE_API_KEY\");\n        if (!this.apiKey) {\n            throw new Error(\"Please set an API key for Google GenerativeAI \" +\n                \"in the environment variable GOOGLE_API_KEY \" +\n                \"or in the `apiKey` field of the \" +\n                \"ChatGoogleGenerativeAI constructor\");\n        }\n        this.safetySettings = fields?.safetySettings ?? this.safetySettings;\n        if (this.safetySettings && this.safetySettings.length > 0) {\n            const safetySettingsSet = new Set(this.safetySettings.map((s) => s.category));\n            if (safetySettingsSet.size !== this.safetySettings.length) {\n                throw new Error(\"The categories in `safetySettings` array must be unique\");\n            }\n        }\n        this.streaming = fields?.streaming ?? this.streaming;\n        this.client = new GenerativeAI(this.apiKey).getGenerativeModel({\n            model: this.modelName,\n            safetySettings: this.safetySettings,\n            generationConfig: {\n                candidateCount: 1,\n                stopSequences: this.stopSequences,\n                maxOutputTokens: this.maxOutputTokens,\n                temperature: this.temperature,\n                topP: this.topP,\n                topK: this.topK,\n            },\n        });\n    }\n    _combineLLMOutput() {\n        return [];\n    }\n    _llmType() {\n        return \"googlegenerativeai\";\n    }\n    async _generate(messages, options, runManager) {\n        const prompt = convertBaseMessagesToContent(messages, this._isMultimodalModel);\n        // Handle streaming\n        if (this.streaming) {\n            const tokenUsage = {};\n            const stream = this._streamResponseChunks(messages, options, runManager);\n            const finalChunks = {};\n            for await (const chunk of stream) {\n                const index = chunk.generationInfo?.completion ?? 0;\n                if (finalChunks[index] === undefined) {\n                    finalChunks[index] = chunk;\n                }\n                else {\n                    finalChunks[index] = finalChunks[index].concat(chunk);\n                }\n            }\n            const generations = Object.entries(finalChunks)\n                .sort(([aKey], [bKey]) => parseInt(aKey, 10) - parseInt(bKey, 10))\n                .map(([_, value]) => value);\n            return { generations, llmOutput: { estimatedTokenUsage: tokenUsage } };\n        }\n        const res = await this.caller.callWithOptions({ signal: options?.signal }, async () => {\n            let output;\n            try {\n                output = await this.client.generateContent({\n                    contents: prompt,\n                });\n                // eslint-disable-next-line @typescript-eslint/no-explicit-any\n            }\n            catch (e) {\n                // TODO: Improve error handling\n                if (e.message?.includes(\"400 Bad Request\")) {\n                    e.status = 400;\n                }\n                throw e;\n            }\n            return output;\n        });\n        const generationResult = mapGenerateContentResultToChatResult(res.response);\n        await runManager?.handleLLMNewToken(generationResult.generations[0].text ?? \"\");\n        return generationResult;\n    }\n    async *_streamResponseChunks(messages, options, runManager) {\n        const prompt = convertBaseMessagesToContent(messages, this._isMultimodalModel);\n        const stream = await this.caller.callWithOptions({ signal: options?.signal }, async () => {\n            const { stream } = await this.client.generateContentStream({\n                contents: prompt,\n            });\n            return stream;\n        });\n        for await (const response of stream) {\n            const chunk = convertResponseContentToChatGenerationChunk(response);\n            if (!chunk) {\n                continue;\n            }\n            yield chunk;\n            await runManager?.handleLLMNewToken(chunk.text ?? \"\");\n        }\n    }\n}\n"],"mappings":"AAAA,SAASA,kBAAkB,IAAIC,YAAY,QAAS,uBAAuB;AAC3E,SAASC,sBAAsB,QAAQ,2BAA2B;AAClE,SAASC,aAAa,QAAS,6CAA6C;AAC5E,SAASC,4BAA4B,EAAEC,2CAA2C,EAAEC,oCAAoC,QAAS,YAAY;AAC7I;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMC,sBAAsB,SAASJ,aAAa,CAAC;EACtD,OAAOK,OAAOA,CAAA,EAAG;IACb,OAAO,oBAAoB;EAC/B;EACA,IAAIC,UAAUA,CAAA,EAAG;IACb,OAAO;MACHC,MAAM,EAAE;IACZ,CAAC;EACL;EACA,IAAIC,kBAAkBA,CAAA,EAAG;IACrB,OAAO,IAAI,CAACC,SAAS,CAACC,QAAQ,CAAC,QAAQ,CAAC;EAC5C;EACAC,WAAWA,CAACC,MAAM,EAAE;IAChB,KAAK,CAACA,MAAM,IAAI,CAAC,CAAC,CAAC;IACnBC,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,iBAAiB,EAAE;MAC3CC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,WAAW,EAAE;MACrCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,aAAa,EAAE;MACvCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC,CAAC,CAAC;IACJL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,iBAAiB,EAAE;MAC3CC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,MAAM,EAAE;MAChCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC,CAAC,CAAC;IACJL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,MAAM,EAAE;MAChCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC,CAAC,CAAC;IACJL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,eAAe,EAAE;MACzCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,gBAAgB,EAAE;MAC1CC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,QAAQ,EAAE;MAClCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,WAAW,EAAE;MACrCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,QAAQ,EAAE;MAClCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACF,IAAI,CAACT,SAAS,GACVG,MAAM,EAAEH,SAAS,EAAEU,OAAO,CAAC,WAAW,EAAE,EAAE,CAAC,IAAI,IAAI,CAACV,SAAS;IACjE,IAAI,CAACW,eAAe,GAAGR,MAAM,EAAEQ,eAAe,IAAI,IAAI,CAACA,eAAe;IACtE,IAAI,IAAI,CAACA,eAAe,IAAI,IAAI,CAACA,eAAe,GAAG,CAAC,EAAE;MAClD,MAAM,IAAIC,KAAK,CAAC,8CAA8C,CAAC;IACnE;IACA,IAAI,CAACC,WAAW,GAAGV,MAAM,EAAEU,WAAW,IAAI,IAAI,CAACA,WAAW;IAC1D,IAAI,IAAI,CAACA,WAAW,KAAK,IAAI,CAACA,WAAW,GAAG,CAAC,IAAI,IAAI,CAACA,WAAW,GAAG,CAAC,CAAC,EAAE;MACpE,MAAM,IAAID,KAAK,CAAC,iDAAiD,CAAC;IACtE;IACA,IAAI,CAACE,IAAI,GAAGX,MAAM,EAAEW,IAAI,IAAI,IAAI,CAACA,IAAI;IACrC,IAAI,IAAI,CAACA,IAAI,IAAI,IAAI,CAACA,IAAI,GAAG,CAAC,EAAE;MAC5B,MAAM,IAAIF,KAAK,CAAC,mCAAmC,CAAC;IACxD;IACA,IAAI,IAAI,CAACE,IAAI,IAAI,IAAI,CAACA,IAAI,GAAG,CAAC,EAAE;MAC5B,MAAM,IAAIF,KAAK,CAAC,yBAAyB,CAAC;IAC9C;IACA,IAAI,CAACG,IAAI,GAAGZ,MAAM,EAAEY,IAAI,IAAI,IAAI,CAACA,IAAI;IACrC,IAAI,IAAI,CAACA,IAAI,IAAI,IAAI,CAACA,IAAI,GAAG,CAAC,EAAE;MAC5B,MAAM,IAAIH,KAAK,CAAC,mCAAmC,CAAC;IACxD;IACA,IAAI,CAACI,aAAa,GAAGb,MAAM,EAAEa,aAAa,IAAI,IAAI,CAACA,aAAa;IAChE,IAAI,CAAClB,MAAM,GAAGK,MAAM,EAAEL,MAAM,IAAIR,sBAAsB,CAAC,gBAAgB,CAAC;IACxE,IAAI,CAAC,IAAI,CAACQ,MAAM,EAAE;MACd,MAAM,IAAIc,KAAK,CAAC,gDAAgD,GAC5D,6CAA6C,GAC7C,kCAAkC,GAClC,oCAAoC,CAAC;IAC7C;IACA,IAAI,CAACK,cAAc,GAAGd,MAAM,EAAEc,cAAc,IAAI,IAAI,CAACA,cAAc;IACnE,IAAI,IAAI,CAACA,cAAc,IAAI,IAAI,CAACA,cAAc,CAACC,MAAM,GAAG,CAAC,EAAE;MACvD,MAAMC,iBAAiB,GAAG,IAAIC,GAAG,CAAC,IAAI,CAACH,cAAc,CAACI,GAAG,CAAEC,CAAC,IAAKA,CAAC,CAACC,QAAQ,CAAC,CAAC;MAC7E,IAAIJ,iBAAiB,CAACK,IAAI,KAAK,IAAI,CAACP,cAAc,CAACC,MAAM,EAAE;QACvD,MAAM,IAAIN,KAAK,CAAC,yDAAyD,CAAC;MAC9E;IACJ;IACA,IAAI,CAACa,SAAS,GAAGtB,MAAM,EAAEsB,SAAS,IAAI,IAAI,CAACA,SAAS;IACpD,IAAI,CAACC,MAAM,GAAG,IAAIrC,YAAY,CAAC,IAAI,CAACS,MAAM,CAAC,CAAC6B,kBAAkB,CAAC;MAC3DC,KAAK,EAAE,IAAI,CAAC5B,SAAS;MACrBiB,cAAc,EAAE,IAAI,CAACA,cAAc;MACnCY,gBAAgB,EAAE;QACdC,cAAc,EAAE,CAAC;QACjBd,aAAa,EAAE,IAAI,CAACA,aAAa;QACjCL,eAAe,EAAE,IAAI,CAACA,eAAe;QACrCE,WAAW,EAAE,IAAI,CAACA,WAAW;QAC7BC,IAAI,EAAE,IAAI,CAACA,IAAI;QACfC,IAAI,EAAE,IAAI,CAACA;MACf;IACJ,CAAC,CAAC;EACN;EACAgB,iBAAiBA,CAAA,EAAG;IAChB,OAAO,EAAE;EACb;EACAC,QAAQA,CAAA,EAAG;IACP,OAAO,oBAAoB;EAC/B;EACA,MAAMC,SAASA,CAACC,QAAQ,EAAEC,OAAO,EAAEC,UAAU,EAAE;IAC3C,MAAMC,MAAM,GAAG7C,4BAA4B,CAAC0C,QAAQ,EAAE,IAAI,CAACnC,kBAAkB,CAAC;IAC9E;IACA,IAAI,IAAI,CAAC0B,SAAS,EAAE;MAChB,MAAMa,UAAU,GAAG,CAAC,CAAC;MACrB,MAAMC,MAAM,GAAG,IAAI,CAACC,qBAAqB,CAACN,QAAQ,EAAEC,OAAO,EAAEC,UAAU,CAAC;MACxE,MAAMK,WAAW,GAAG,CAAC,CAAC;MACtB,WAAW,MAAMC,KAAK,IAAIH,MAAM,EAAE;QAC9B,MAAMI,KAAK,GAAGD,KAAK,CAACE,cAAc,EAAEC,UAAU,IAAI,CAAC;QACnD,IAAIJ,WAAW,CAACE,KAAK,CAAC,KAAKG,SAAS,EAAE;UAClCL,WAAW,CAACE,KAAK,CAAC,GAAGD,KAAK;QAC9B,CAAC,MACI;UACDD,WAAW,CAACE,KAAK,CAAC,GAAGF,WAAW,CAACE,KAAK,CAAC,CAACI,MAAM,CAACL,KAAK,CAAC;QACzD;MACJ;MACA,MAAMM,WAAW,GAAG5C,MAAM,CAAC6C,OAAO,CAACR,WAAW,CAAC,CAC1CS,IAAI,CAAC,CAAC,CAACC,IAAI,CAAC,EAAE,CAACC,IAAI,CAAC,KAAKC,QAAQ,CAACF,IAAI,EAAE,EAAE,CAAC,GAAGE,QAAQ,CAACD,IAAI,EAAE,EAAE,CAAC,CAAC,CACjE/B,GAAG,CAAC,CAAC,CAACiC,CAAC,EAAE7C,KAAK,CAAC,KAAKA,KAAK,CAAC;MAC/B,OAAO;QAAEuC,WAAW;QAAEO,SAAS,EAAE;UAAEC,mBAAmB,EAAElB;QAAW;MAAE,CAAC;IAC1E;IACA,MAAMmB,GAAG,GAAG,MAAM,IAAI,CAACC,MAAM,CAACC,eAAe,CAAC;MAAEC,MAAM,EAAEzB,OAAO,EAAEyB;IAAO,CAAC,EAAE,YAAY;MACnF,IAAIC,MAAM;MACV,IAAI;QACAA,MAAM,GAAG,MAAM,IAAI,CAACnC,MAAM,CAACoC,eAAe,CAAC;UACvCC,QAAQ,EAAE1B;QACd,CAAC,CAAC;QACF;MACJ,CAAC,CACD,OAAO2B,CAAC,EAAE;QACN;QACA,IAAIA,CAAC,CAACC,OAAO,EAAEhE,QAAQ,CAAC,iBAAiB,CAAC,EAAE;UACxC+D,CAAC,CAACE,MAAM,GAAG,GAAG;QAClB;QACA,MAAMF,CAAC;MACX;MACA,OAAOH,MAAM;IACjB,CAAC,CAAC;IACF,MAAMM,gBAAgB,GAAGzE,oCAAoC,CAAC+D,GAAG,CAACW,QAAQ,CAAC;IAC3E,MAAMhC,UAAU,EAAEiC,iBAAiB,CAACF,gBAAgB,CAACnB,WAAW,CAAC,CAAC,CAAC,CAACsB,IAAI,IAAI,EAAE,CAAC;IAC/E,OAAOH,gBAAgB;EAC3B;EACA,OAAO3B,qBAAqBA,CAACN,QAAQ,EAAEC,OAAO,EAAEC,UAAU,EAAE;IACxD,MAAMC,MAAM,GAAG7C,4BAA4B,CAAC0C,QAAQ,EAAE,IAAI,CAACnC,kBAAkB,CAAC;IAC9E,MAAMwC,MAAM,GAAG,MAAM,IAAI,CAACmB,MAAM,CAACC,eAAe,CAAC;MAAEC,MAAM,EAAEzB,OAAO,EAAEyB;IAAO,CAAC,EAAE,YAAY;MACtF,MAAM;QAAErB;MAAO,CAAC,GAAG,MAAM,IAAI,CAACb,MAAM,CAAC6C,qBAAqB,CAAC;QACvDR,QAAQ,EAAE1B;MACd,CAAC,CAAC;MACF,OAAOE,MAAM;IACjB,CAAC,CAAC;IACF,WAAW,MAAM6B,QAAQ,IAAI7B,MAAM,EAAE;MACjC,MAAMG,KAAK,GAAGjD,2CAA2C,CAAC2E,QAAQ,CAAC;MACnE,IAAI,CAAC1B,KAAK,EAAE;QACR;MACJ;MACA,MAAMA,KAAK;MACX,MAAMN,UAAU,EAAEiC,iBAAiB,CAAC3B,KAAK,CAAC4B,IAAI,IAAI,EAAE,CAAC;IACzD;EACJ;AACJ","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}